{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab09_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SfaKzDfXofr",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this lab we will illustrate some of the ideas of word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQl3qC2bXlhu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "959194aa-a5af-4a84-e48c-581487b89553"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo-bhEnQcEuI",
        "colab_type": "text"
      },
      "source": [
        "## Set up -- getting the data and word embeddings\n",
        "As in the cats vs dogs lab, I have shared the necessary files with you in a Google drive folder.  To get the data into colab, do these steps:\n",
        "\n",
        "1. Sign into drive.google.com\n",
        "2. Click on \"Shared with me\" on the left side of the screen\n",
        "3. Right click on the stat344ne_imdb folder and select \"Add shortcut to Drive\", and choose \"My Drive\" for the shortcut location.\n",
        "4. Run the code cell below and click on the link that is displayed.  It will pop up a new browser tab where you have to authorize Colab to access your google drive.  Then, copy the sequence of numbers and letters that is displayed and paste it in the space that shows up in the code cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lw9KLykcaci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6e1d00ba-c8e8-4d07-92bb-87d278397be0"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XQqHbnEhS9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "d6f905e8-f053-4e7f-e5ee-713d71caafcd"
      },
      "source": [
        "os.mkdir(\"/content/stat344ne_imdb/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7bf0c56051fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/stat344ne_imdb/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/stat344ne_imdb/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79C3EFu5cdyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -uq \"/content/drive/My Drive/stat344ne_imdb/glove.6B.50d.txt.zip\" -d \"/content/stat344ne_imdb/glove/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-tcVwlOkOs-",
        "colab_type": "text"
      },
      "source": [
        "### Load word embeddings\n",
        "\n",
        "We are working here with the GloVe (**Glo**bal **Ve**ctors for word representation) word embeddings.  The word embeddings are stored in a large text file.  Each line represents one word, with corresponding coefficients giving its embedding (that is, its representation).  In the particular version we are working with, the embedding dimension is 50.  I chose this dimension to use because it is the smallest embedding dimension available (so it has the smallest file size).  Other options for the dimension are 100, 200, and 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzDJPuishPVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8198477-8be8-4d51-f059-146eb40aa0b0"
      },
      "source": [
        "glove_dir = \"/content/stat344ne_imdb/glove\"\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNCjQLSbnQbU",
        "colab_type": "text"
      },
      "source": [
        "Here are the first 20 words in the embeddings dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhYOJ-00j5oh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "74aece0d-f27b-4d97-ee8b-59ccdfdde8d8"
      },
      "source": [
        "list(embeddings_index)[:20]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " ',',\n",
              " '.',\n",
              " 'of',\n",
              " 'to',\n",
              " 'and',\n",
              " 'in',\n",
              " 'a',\n",
              " '\"',\n",
              " \"'s\",\n",
              " 'for',\n",
              " '-',\n",
              " 'that',\n",
              " 'on',\n",
              " 'is',\n",
              " 'was',\n",
              " 'said',\n",
              " 'with',\n",
              " 'he',\n",
              " 'as']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYCGR0X2neJw",
        "colab_type": "text"
      },
      "source": [
        "Here is the embedding of the word 'cat'.  It is a vector of 50 real numbers since we are working with embedding dimension 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9OTBSNikDxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b24abd06-f68a-4fd3-e43b-4e4a02049bfd"
      },
      "source": [
        "embeddings_index['cat']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.45281 , -0.50108 , -0.53714 , -0.015697,  0.22191 ,  0.54602 ,\n",
              "       -0.67301 , -0.6891  ,  0.63493 , -0.19726 ,  0.33685 ,  0.7735  ,\n",
              "        0.90094 ,  0.38488 ,  0.38367 ,  0.2657  , -0.08057 ,  0.61089 ,\n",
              "       -1.2894  , -0.22313 , -0.61578 ,  0.21697 ,  0.35614 ,  0.44499 ,\n",
              "        0.60885 , -1.1633  , -1.1579  ,  0.36118 ,  0.10466 , -0.78325 ,\n",
              "        1.4352  ,  0.18629 , -0.26112 ,  0.83275 , -0.23123 ,  0.32481 ,\n",
              "        0.14485 , -0.44552 ,  0.33497 , -0.95946 , -0.097479,  0.48138 ,\n",
              "       -0.43352 ,  0.69455 ,  0.91043 , -0.28173 ,  0.41637 , -1.2609  ,\n",
              "        0.71278 ,  0.23782 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efOEfIFoYiw",
        "colab_type": "text"
      },
      "source": [
        "### Cosine Similarity\n",
        "\n",
        "In our videos today, we saw that the similarity of two vectors can be measured by the cosine of the angle between the vectors, which is closely related to the inner product of the vectors.  We also saw that with a one-hot encoding, the similarity for two different words will be 0.\n",
        "\n",
        "In the videos, I gave the relation $v \\cdot w = ||v|| \\, ||w|| \\, cos(\\theta)$, where $\\theta$ is the angle between the vectors $v$ and $w$.  We can rearrange this to obtain the **cosine similarity score**:\n",
        "\n",
        "$$cos(\\theta) = \\frac{v \\cdot w}{||v|| \\, ||w||}.$$\n",
        "\n",
        "#### 1. Implement a function to calculate the similarity of two vectors $v$ and $w$ using the cosine similarity score.\n",
        "\n",
        "You may use the functions `np.dot`, `np.sqrt`, and/or `np.linalg.norm`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnRmOzukmkOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_similarity(v, w):\n",
        "  '''\n",
        "  Calculate cosine similarity of vectors v and w\n",
        "\n",
        "  Arguments:\n",
        "   - v: column vector of shape (d, 1)\n",
        "   - w: column vector of shape (d, 1)\n",
        "  \n",
        "  Return:\n",
        "   - cosine similarity of v and w\n",
        "  '''\n",
        "  # add your calculation here.  You can add more lines if it's helpful\n",
        "  result = np.dot(v.T, w) / (np.linalg.norm(v) * np.linalg.norm(w))\n",
        "  return(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHwtIhVTrPCk",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Find the cosine similarity of the word embeddings for \"cat\" and \"dog\".\n",
        "Note that you will need to `reshape` the embeddings to have shape (50, 1) for your function to work correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx_Lnp_drNqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9007e373-c438-412c-cb05-da198ca6bcdd"
      },
      "source": [
        "cos_similarity(\n",
        "    embeddings_index['cat'].reshape((50,1)),\n",
        "    embeddings_index['dog'].reshape((50,1))\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92180043]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaZfTPhQr3yg",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Find the cosine similarity of the word embeddings for \"cat\" and \"antarctica\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sODSC1CrpwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2f2cadf6-6e44-4a28-bde1-0438ac6759de"
      },
      "source": [
        "cos_similarity(\n",
        "    embeddings_index['cat'].reshape((50,1)),\n",
        "    embeddings_index['antarctica'].reshape((50,1))\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.19274694]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py_9IzuOG5Ce",
        "colab_type": "text"
      },
      "source": [
        "#### 4. An interesting, amazing, and actually sortof sinister, aspect of word embeddings is that arithmetic differences of word embeddings tend to respect analogies.  For example, the difference between the embedding of 'paris' and 'france' is similar to the difference between the embedding of 'rome' and 'italy'.  The interpretation of this is that if $e_{paris}$ is the embedding vector of 'paris' and $e_{france}$ is the embedding vector of 'france', then the difference $e_{paris} - e_{france}$ represents a meaningful direction in the vector space, encapsulating the relationship between a city and a country it is in.  We will explore this more in a future lab.  For now, calculate the cosine similarity between the difference $e_{paris} - e_{france}$ and the difference $e_{rome} - e_{italy}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv9fzKyEHth7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "167457df-3023-4478-ecae-037390b858ea"
      },
      "source": [
        "cos_similarity(\n",
        "    embeddings_index['paris'].reshape((50,1)) - embeddings_index['france'].reshape((50,1)),\n",
        "    embeddings_index['rome'].reshape((50,1)) - embeddings_index['italy'].reshape((50,1))\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.67514807]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}